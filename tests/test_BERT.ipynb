{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88f00b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /opt/anaconda3\n",
      "\n",
      "  added / updated specs:\n",
      "    - transformers\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    boltons-23.0.0             |   py39hecd8cb5_0         423 KB\n",
      "    conda-23.3.0               |   py39hecd8cb5_0         964 KB\n",
      "    huggingface_hub-0.13.3     |             py_0         195 KB  huggingface\n",
      "    ninja-1.10.2               |       hecd8cb5_5           9 KB\n",
      "    ninja-base-1.10.2          |       haf03e11_5         118 KB\n",
      "    pytorch-1.12.1             |cpu_py39ha26b6ec_0        53.7 MB\n",
      "    tokenizers-0.13.1          |           py39_0         3.3 MB  huggingface\n",
      "    transformers-4.24.0        |   py39hecd8cb5_0         4.7 MB\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        63.4 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  boltons            pkgs/main/osx-64::boltons-23.0.0-py39hecd8cb5_0 \n",
      "  huggingface_hub    huggingface/noarch::huggingface_hub-0.13.3-py_0 \n",
      "  ninja              pkgs/main/osx-64::ninja-1.10.2-hecd8cb5_5 \n",
      "  ninja-base         pkgs/main/osx-64::ninja-base-1.10.2-haf03e11_5 \n",
      "  pytorch            pkgs/main/osx-64::pytorch-1.12.1-cpu_py39ha26b6ec_0 \n",
      "  tokenizers         huggingface/osx-64::tokenizers-0.13.1-py39_0 \n",
      "  transformers       pkgs/main/osx-64::transformers-4.24.0-py39hecd8cb5_0 \n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  conda                               23.1.0-py39hecd8cb5_0 --> 23.3.0-py39hecd8cb5_0 \n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "boltons-23.0.0       | 423 KB    |                                       |   0% \n",
      "pytorch-1.12.1       | 53.7 MB   |                                       |   0% \u001b[A\n",
      "\n",
      "tokenizers-0.13.1    | 3.3 MB    |                                       |   0% \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "conda-23.3.0         | 964 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "ninja-base-1.10.2    | 118 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ninja-1.10.2         | 9 KB      |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "huggingface_hub-0.13 | 195 KB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "transformers-4.24.0  | 4.7 MB    |                                       |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "conda-23.3.0         | 964 KB    | 6                                     |   2% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "boltons-23.0.0       | 423 KB    | #3                                    |   4% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ninja-1.10.2         | 9 KB      | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "pytorch-1.12.1       | 53.7 MB   |                                       |   0% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "boltons-23.0.0       | 423 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "pytorch-1.12.1       | 53.7 MB   | #                                     |   3% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ninja-1.10.2         | 9 KB      | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "transformers-4.24.0  | 4.7 MB    | 1                                     |   0% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "pytorch-1.12.1       | 53.7 MB   | #6                                    |   5% \u001b[A\n",
      "\n",
      "\n",
      "conda-23.3.0         | 964 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "conda-23.3.0         | 964 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\n",
      "pytorch-1.12.1       | 53.7 MB   | ###8                                  |  10% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "transformers-4.24.0  | 4.7 MB    | #########                             |  24% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "pytorch-1.12.1       | 53.7 MB   | ####9                                 |  13% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "transformers-4.24.0  | 4.7 MB    | ###############                       |  41% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "huggingface_hub-0.13 | 195 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "huggingface_hub-0.13 | 195 KB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "pytorch-1.12.1       | 53.7 MB   | ######                                |  16% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "transformers-4.24.0  | 4.7 MB    | ####################                  |  54% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "pytorch-1.12.1       | 53.7 MB   | #######1                              |  19% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "transformers-4.24.0  | 4.7 MB    | ########################5             |  66% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "pytorch-1.12.1       | 53.7 MB   | ########6                             |  23% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "transformers-4.24.0  | 4.7 MB    | ################################      |  87% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "pytorch-1.12.1       | 53.7 MB   | ##########                            |  27% \u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "transformers-4.24.0  | 4.7 MB    | ##################################### | 100% \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "pytorch-1.12.1       | 53.7 MB   | ###########2                          |  30% \u001b[A\n",
      "pytorch-1.12.1       | 53.7 MB   | #############9                        |  38% \u001b[A\n",
      "pytorch-1.12.1       | 53.7 MB   | ###############8                      |  43% \u001b[A\n",
      "pytorch-1.12.1       | 53.7 MB   | #################7                    |  48% \u001b[A\n",
      "pytorch-1.12.1       | 53.7 MB   | ###################5                  |  53% \u001b[A\n",
      "pytorch-1.12.1       | 53.7 MB   | #####################4                |  58% \u001b[A\n",
      "pytorch-1.12.1       | 53.7 MB   | #######################3              |  63% \u001b[A\n",
      "pytorch-1.12.1       | 53.7 MB   | #########################1            |  68% \u001b[A\n",
      "pytorch-1.12.1       | 53.7 MB   | ###########################1          |  73% \u001b[A\n",
      "pytorch-1.12.1       | 53.7 MB   | #############################3        |  79% \u001b[A\n",
      "pytorch-1.12.1       | 53.7 MB   | ###############################2      |  84% \u001b[A\n",
      "pytorch-1.12.1       | 53.7 MB   | #################################4    |  90% \u001b[A\n",
      "pytorch-1.12.1       | 53.7 MB   | ###################################2  |  95% \u001b[A\n",
      "pytorch-1.12.1       | 53.7 MB   | ##################################### | 100% \u001b[A\n",
      "\n",
      "tokenizers-0.13.1    | 3.3 MB    | ##################################### | 100% \u001b[A\u001b[A\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                                                                \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda install -c huggingface transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9e8a12d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7dcb7f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ehsanaghaei/SecureBERT were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at ehsanaghaei/SecureBERT and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained(\"ehsanaghaei/SecureBERT\")\n",
    "model = RobertaModel.from_pretrained(\"ehsanaghaei/SecureBERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8cc91dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\"What is this\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d8653b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.0187,  0.0004, -0.0178,  ...,  0.0001, -0.0278, -0.0067],\n",
       "         [-0.0208, -0.0128,  0.0045,  ...,  0.0359,  0.0014,  0.0123],\n",
       "         [ 0.0630,  0.0359, -0.0248,  ..., -0.0445,  0.0083,  0.0160],\n",
       "         [ 0.0126, -0.0022, -0.0429,  ...,  0.0464,  0.0074,  0.0245],\n",
       "         [-0.0189,  0.0005, -0.0178,  ...,  0.0002, -0.0284, -0.0070]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[ 3.6992e-02, -1.4735e-02,  4.9862e-02,  2.1527e-02, -2.2854e-02,\n",
       "         -1.8805e-02,  9.6335e-02, -5.8453e-03,  3.0925e-02, -4.9054e-04,\n",
       "         -1.8574e-02, -4.5984e-02, -7.1592e-03,  4.1582e-03,  1.8777e-02,\n",
       "         -5.5371e-02, -3.1295e-02,  3.3802e-02, -9.7872e-02, -3.2861e-02,\n",
       "          9.8854e-03,  1.6773e-02,  9.7344e-03,  2.5315e-02, -1.7604e-02,\n",
       "          2.6204e-02,  2.3392e-02, -4.5740e-03, -4.2295e-02, -3.9669e-02,\n",
       "          5.4804e-03, -3.7192e-02, -7.5921e-04,  5.9087e-02, -1.8748e-02,\n",
       "         -4.0021e-02,  2.4219e-02, -1.3109e-02,  3.1754e-02,  3.9014e-03,\n",
       "         -3.3940e-03, -1.4062e-02,  2.8462e-02,  6.8413e-03,  3.1715e-02,\n",
       "          1.3092e-02,  1.3187e-02,  4.2338e-02, -1.7651e-03, -7.5403e-04,\n",
       "         -1.1941e-02,  3.9709e-02, -1.3551e-02, -1.7899e-02,  7.3102e-03,\n",
       "         -1.8017e-02, -1.2017e-02,  4.2138e-02, -4.4566e-02,  4.1487e-02,\n",
       "          1.6204e-02,  4.9978e-02, -5.5199e-03,  2.2913e-02,  6.5681e-03,\n",
       "         -3.8763e-03, -3.6468e-02,  1.6868e-02,  1.6161e-02, -3.6699e-02,\n",
       "         -1.9491e-02, -1.5810e-02,  5.1285e-02, -2.3591e-02, -7.9418e-02,\n",
       "         -5.2588e-02,  4.8522e-02,  8.9079e-03, -1.0330e-01, -7.5208e-03,\n",
       "         -3.1232e-02,  1.6969e-02,  7.6309e-03, -2.9582e-02,  4.2930e-02,\n",
       "          3.0747e-02,  2.8178e-02, -1.8239e-02,  1.1864e-02, -4.8038e-02,\n",
       "         -8.0987e-03,  3.8987e-02, -2.9280e-03,  6.7486e-03,  9.6481e-03,\n",
       "          9.1748e-04,  1.7266e-02,  1.3980e-02, -2.4795e-02,  1.6888e-02,\n",
       "          2.2258e-02,  8.3504e-03,  1.1702e-02,  4.8113e-02,  1.6471e-02,\n",
       "          1.4894e-03, -5.4465e-02, -1.9656e-02, -2.5915e-02,  7.9248e-02,\n",
       "          1.6517e-02, -4.1549e-02, -3.4557e-02,  3.9133e-02,  2.6303e-02,\n",
       "          6.7916e-03, -5.1164e-02, -2.0129e-02,  1.2197e-02,  5.4723e-02,\n",
       "         -2.5524e-02,  1.7942e-02,  1.7319e-02, -4.3914e-02, -4.5905e-02,\n",
       "          4.7611e-02, -1.0313e-02,  2.5971e-02,  5.4863e-02, -1.6570e-02,\n",
       "          6.6184e-04,  5.1897e-02, -2.6214e-02, -4.2209e-02,  5.0114e-03,\n",
       "         -5.8214e-03,  1.3098e-02,  1.9333e-02,  1.8854e-02,  6.5608e-03,\n",
       "         -2.0166e-02,  1.8563e-02, -1.9110e-02, -1.8358e-02, -2.9849e-02,\n",
       "         -4.3164e-02,  3.7408e-02,  1.2196e-02,  1.3743e-02, -3.3003e-02,\n",
       "          1.9361e-02, -4.6123e-02, -2.9711e-02, -9.1788e-03,  5.5598e-02,\n",
       "          9.3514e-03, -4.0560e-02,  2.8485e-02,  3.7625e-02,  4.0842e-02,\n",
       "          1.0024e-02, -1.9778e-02,  3.6923e-03, -4.9196e-02, -1.9746e-02,\n",
       "          3.9540e-03,  7.8446e-02, -7.4160e-02,  3.6615e-02, -8.7078e-02,\n",
       "         -3.4428e-02,  4.4358e-02, -1.1483e-02,  3.5681e-02,  4.3696e-02,\n",
       "          3.1874e-02,  9.9325e-03,  1.7191e-02,  6.4563e-02,  3.5480e-02,\n",
       "         -1.2787e-02, -5.1021e-03,  3.5603e-02, -9.7921e-03,  9.5061e-03,\n",
       "          6.3332e-03, -2.7517e-02,  6.9734e-03, -2.1715e-02, -4.7135e-02,\n",
       "         -1.7820e-02, -2.8906e-02, -1.2965e-02,  1.6281e-02, -5.6020e-03,\n",
       "         -2.5569e-02,  6.5302e-02,  3.7393e-04,  2.2611e-03,  1.6156e-02,\n",
       "         -7.7866e-02, -3.2990e-02, -2.4839e-02, -4.0910e-03,  6.7465e-02,\n",
       "          2.7528e-02,  1.1216e-02, -4.0726e-02,  3.1975e-02,  3.9319e-02,\n",
       "          1.0722e-02, -2.8120e-02, -8.7795e-02,  5.9881e-03,  3.1469e-02,\n",
       "          2.2027e-02, -4.6494e-02,  1.5659e-02,  1.5718e-02, -4.6328e-02,\n",
       "          6.0735e-02,  7.9217e-03,  5.1731e-02, -7.0634e-02,  2.6784e-02,\n",
       "          2.9360e-02,  7.8445e-02, -2.2811e-02, -1.0702e-02,  5.6748e-02,\n",
       "          4.1784e-02,  1.1424e-02, -1.1762e-02,  2.4734e-02,  2.5857e-02,\n",
       "          2.0350e-02, -1.8336e-02, -3.7028e-02, -2.3369e-02,  6.5210e-02,\n",
       "          4.0373e-02,  2.6583e-02, -3.8890e-03, -3.0087e-02, -1.8191e-02,\n",
       "         -5.0642e-02, -2.4365e-02,  1.8171e-02, -4.6639e-02, -6.1400e-02,\n",
       "          2.0210e-02, -1.4306e-02,  4.1979e-02, -3.0349e-02, -9.3089e-02,\n",
       "          5.9257e-03,  1.0103e-04,  4.9423e-02,  2.2764e-02,  6.2074e-02,\n",
       "          7.4670e-02, -9.1060e-02,  2.2339e-02, -1.3043e-02, -2.6161e-02,\n",
       "          2.4656e-02, -1.2792e-03, -7.3485e-03, -1.5265e-04, -3.1837e-02,\n",
       "         -1.6315e-02, -5.0165e-02, -1.5255e-02, -3.5649e-02, -8.2129e-03,\n",
       "          4.4602e-02, -1.9934e-02, -2.9864e-02,  7.9976e-03,  2.8514e-02,\n",
       "          3.2486e-02,  2.0841e-02,  3.1148e-03,  1.4850e-02,  1.3348e-02,\n",
       "          3.7276e-03,  7.8210e-04,  9.0616e-03, -2.0763e-02, -3.0571e-02,\n",
       "          3.5463e-02, -4.1328e-02, -4.0631e-02, -1.4313e-02,  3.7305e-02,\n",
       "          1.5922e-02, -1.1668e-02, -4.6421e-02, -1.0831e-02,  1.9313e-02,\n",
       "          5.4183e-03,  1.6495e-02, -4.2914e-03, -3.2930e-02, -1.0270e-02,\n",
       "          1.0071e-02,  2.6380e-02,  4.2796e-02,  1.4039e-02,  6.2057e-03,\n",
       "         -5.8440e-02,  3.3153e-02, -3.9007e-02, -1.9605e-02,  1.8402e-02,\n",
       "          3.7436e-03, -5.6055e-02,  3.5736e-02,  2.6735e-03,  1.4727e-02,\n",
       "         -1.0900e-01,  7.8447e-02, -5.9145e-02, -6.0131e-02, -1.2623e-02,\n",
       "         -5.9769e-02, -2.3875e-02, -2.5872e-02, -3.9050e-02,  6.9759e-02,\n",
       "         -2.6028e-02,  5.7730e-02,  5.1308e-02, -3.0648e-02,  5.5169e-02,\n",
       "         -4.2481e-02,  3.3155e-02,  6.4320e-02,  9.7129e-02,  6.7311e-02,\n",
       "          5.4381e-02, -2.4642e-02, -3.9468e-02,  7.7136e-03,  1.3348e-02,\n",
       "          3.6542e-02, -4.8236e-02,  9.4060e-02,  1.9244e-02,  5.9013e-03,\n",
       "          4.4296e-02,  4.2334e-02,  1.2451e-01,  3.6251e-02, -2.3082e-02,\n",
       "         -7.7910e-03,  1.0571e-02,  5.2244e-02,  2.6742e-03, -2.4236e-02,\n",
       "          2.3373e-02, -1.4826e-02, -5.5028e-02, -1.9852e-02, -1.0767e-02,\n",
       "          2.5796e-02,  1.8128e-03,  6.9517e-02, -6.0239e-02, -6.2319e-02,\n",
       "         -5.6044e-02,  9.6361e-03, -4.4763e-02,  1.2453e-02,  4.9698e-02,\n",
       "         -6.2024e-03,  2.9358e-03, -3.9149e-02,  6.5145e-02, -6.9279e-02,\n",
       "         -4.3419e-02,  2.2076e-02,  6.7103e-02,  2.8339e-02, -5.8879e-04,\n",
       "         -8.1570e-03, -5.9502e-02, -9.4100e-03, -3.7396e-03,  2.9245e-02,\n",
       "         -1.5222e-02, -3.5384e-02, -4.6629e-02,  6.1197e-02,  2.1722e-02,\n",
       "          1.9482e-02,  1.8661e-02,  3.7642e-02, -1.5271e-02,  7.1229e-02,\n",
       "          6.4747e-04, -7.3619e-02, -1.4395e-02, -7.0900e-02, -4.8270e-02,\n",
       "          4.1012e-02, -1.5531e-02, -2.9123e-02,  5.1159e-02,  7.6766e-02,\n",
       "         -3.8993e-02, -1.1076e-02, -3.5733e-02, -2.1071e-02, -2.3768e-02,\n",
       "         -2.0234e-02, -3.4650e-02, -1.4052e-02,  7.4621e-02, -2.4443e-02,\n",
       "          3.6467e-02, -4.8334e-02,  1.2882e-02, -6.0245e-02,  2.9804e-02,\n",
       "          6.1510e-02, -7.4257e-02,  6.6642e-02,  3.1687e-02, -1.7999e-02,\n",
       "         -2.8621e-03,  8.6851e-03, -5.2737e-02, -8.1877e-02,  1.1156e-02,\n",
       "          7.6130e-03, -1.4518e-02,  6.7471e-02,  1.3146e-02, -1.0823e-03,\n",
       "          3.8300e-02,  1.3777e-02, -5.2075e-02,  1.1719e-02, -1.9360e-02,\n",
       "          7.9192e-02,  1.0515e-01, -4.9030e-02,  3.4215e-03, -5.0470e-02,\n",
       "         -3.0485e-02,  2.5910e-02,  4.1977e-03, -1.7056e-03,  2.4725e-02,\n",
       "          2.4379e-02, -4.2785e-02,  2.4861e-02, -1.8377e-02,  2.6697e-03,\n",
       "          5.2984e-02,  7.9290e-02,  1.1198e-02, -8.1817e-02,  4.8367e-02,\n",
       "          2.4553e-02,  5.6099e-03,  6.1614e-02, -3.5902e-02,  9.2067e-02,\n",
       "          7.2690e-02,  6.9980e-04, -1.3634e-02, -2.4740e-02, -2.0934e-02,\n",
       "          2.5962e-02,  1.1574e-02,  3.4257e-02, -3.3685e-02,  4.7397e-02,\n",
       "          8.6697e-03, -3.1183e-02, -2.6258e-02, -5.0579e-02,  3.2411e-02,\n",
       "          3.6830e-02, -5.2548e-02, -7.0511e-03, -4.3877e-02,  1.8001e-02,\n",
       "          7.5421e-02, -7.5585e-03,  5.6871e-02, -4.4480e-02, -2.3398e-02,\n",
       "          3.7979e-03, -1.7305e-02,  3.4674e-02, -3.4572e-02,  7.5978e-03,\n",
       "          1.3872e-02, -1.4156e-02, -1.7932e-02, -4.2981e-02, -3.5890e-02,\n",
       "         -2.4906e-02, -5.0940e-02, -2.7880e-02, -2.6488e-02,  6.0839e-02,\n",
       "         -7.2301e-03,  1.1997e-03,  1.4766e-02,  9.5408e-02, -5.3475e-02,\n",
       "          5.0875e-03, -7.0568e-02, -2.3548e-02, -1.7973e-02, -1.8829e-02,\n",
       "         -1.7360e-02, -8.5177e-03,  1.2952e-02,  3.3127e-02, -3.4856e-02,\n",
       "         -1.4103e-02,  5.9024e-02,  9.9112e-02,  1.7014e-02, -3.1417e-02,\n",
       "          6.5454e-03, -6.8588e-02, -3.0513e-02,  6.8709e-03,  2.0464e-02,\n",
       "         -3.5331e-02, -4.3891e-02, -3.3549e-02,  2.2222e-02,  2.0243e-02,\n",
       "          7.1542e-02,  5.1764e-03,  6.3565e-02, -4.2049e-02,  2.9244e-02,\n",
       "          1.2015e-02, -2.3984e-02, -2.1002e-02,  3.8887e-02, -5.7259e-02,\n",
       "         -7.9715e-02, -5.6960e-02,  5.5815e-03, -2.3118e-02,  3.4024e-02,\n",
       "         -5.5931e-02, -5.6998e-03,  5.9705e-02, -4.3890e-02,  4.6082e-02,\n",
       "          4.5028e-02, -4.4214e-03, -3.8280e-02, -5.9569e-02,  5.7813e-03,\n",
       "         -4.3419e-02, -3.8973e-02,  2.6051e-02, -6.0042e-02, -2.2701e-02,\n",
       "         -8.5108e-02, -2.7876e-02, -1.7454e-02, -3.1823e-02, -1.2539e-02,\n",
       "         -4.9521e-02, -6.6503e-02,  3.6478e-03, -2.7599e-02, -3.0442e-02,\n",
       "          7.1429e-02, -2.1039e-02,  2.1325e-02,  9.0947e-02,  3.3425e-02,\n",
       "          9.5371e-03,  3.0530e-02, -1.0568e-01,  5.3625e-03,  4.0567e-02,\n",
       "         -1.0575e-01,  1.8104e-02,  1.0723e-01, -6.5587e-02,  1.9319e-02,\n",
       "          4.8860e-02,  4.5702e-02, -8.4156e-03,  6.1350e-03,  6.9355e-04,\n",
       "          1.5487e-03, -7.5371e-03, -3.3589e-02,  2.0189e-02, -6.6314e-02,\n",
       "          3.9479e-02, -4.4244e-02,  4.9100e-02,  3.2024e-02, -1.0413e-02,\n",
       "         -7.3462e-02, -7.0680e-03,  3.2799e-02, -6.0086e-02, -5.5302e-02,\n",
       "         -7.3339e-03,  4.2742e-02, -1.4388e-03, -3.5295e-02,  1.0855e-02,\n",
       "         -7.2524e-02, -1.9295e-03,  2.9941e-02,  1.6473e-02, -4.8679e-02,\n",
       "         -1.0295e-03,  8.0448e-03, -1.2553e-02, -3.6559e-02,  2.1074e-02,\n",
       "          1.0928e-02,  1.5244e-02,  3.8660e-02, -6.4957e-02,  3.3376e-02,\n",
       "         -5.3655e-03,  4.5622e-02, -7.3985e-02,  1.8385e-02, -1.0401e-02,\n",
       "         -5.4602e-03,  1.8055e-02, -2.0560e-02, -3.2151e-02, -4.9440e-02,\n",
       "          4.2748e-02,  2.0579e-03, -1.0056e-02,  1.5131e-02,  6.9785e-03,\n",
       "         -7.4487e-03,  3.8355e-02, -4.7901e-02, -6.7209e-03, -3.7746e-02,\n",
       "         -5.4955e-02,  2.2607e-02,  4.8868e-02,  6.4503e-02,  3.8903e-02,\n",
       "          8.3492e-03, -1.3721e-02, -1.8469e-02,  3.1624e-02, -2.8517e-02,\n",
       "         -3.0002e-02, -2.1171e-02,  2.1811e-02,  2.9890e-02, -2.4357e-02,\n",
       "          3.5763e-03, -2.6304e-02, -8.8331e-03, -2.5981e-02, -3.1317e-02,\n",
       "          3.2344e-02,  2.8326e-03,  1.7131e-02,  2.3085e-02, -9.8153e-02,\n",
       "         -1.8479e-02, -2.8175e-02,  2.2503e-03,  6.5398e-02,  2.7360e-03,\n",
       "         -1.3556e-02,  1.7292e-02,  2.2668e-02,  4.6140e-02,  3.6074e-03,\n",
       "          3.8568e-02,  2.9606e-02,  1.4036e-02, -4.9623e-02,  4.6560e-02,\n",
       "         -3.6670e-03, -3.3715e-02, -3.6051e-02,  1.3939e-04, -4.4184e-02,\n",
       "          4.7385e-02, -2.2362e-02,  3.4663e-03,  7.8438e-02, -4.9435e-03,\n",
       "         -2.0589e-02, -1.5798e-03,  1.5887e-03,  4.5841e-03,  2.5475e-02,\n",
       "          3.8448e-02, -2.2926e-02, -4.8304e-02, -1.8329e-02, -4.0893e-02,\n",
       "         -3.9649e-03, -5.5318e-02, -9.8882e-03, -6.2603e-03, -4.2669e-02,\n",
       "          2.1125e-02,  3.6835e-02,  4.3375e-02, -2.1577e-02,  1.0749e-03,\n",
       "          1.5209e-02, -1.5271e-02,  3.5824e-02,  3.2540e-02,  3.1978e-02,\n",
       "         -4.1244e-02, -3.0764e-02,  9.1933e-03, -9.9575e-03,  5.2734e-02,\n",
       "          6.9142e-02, -1.9404e-02,  3.5244e-02,  5.3087e-02,  4.8492e-02,\n",
       "         -1.8523e-02, -1.8443e-02, -7.7249e-03,  1.0459e-02,  5.7644e-02,\n",
       "         -2.8554e-02, -2.8966e-02, -3.8804e-02, -4.0279e-02, -4.1455e-02,\n",
       "         -6.8212e-02,  2.5321e-02, -9.9055e-03,  2.5351e-02,  2.1081e-02,\n",
       "         -1.2904e-02,  1.3712e-02, -4.5945e-02, -4.2265e-02, -1.4835e-02,\n",
       "         -4.1119e-02, -5.3356e-02,  1.4818e-02,  4.6315e-02, -2.8751e-02,\n",
       "         -7.7048e-02,  1.7778e-02, -1.9442e-02]], grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5804f225",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0431455",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9f0138de",
   "metadata": {},
   "outputs": [],
   "source": [
    "finbert = BertForSequenceClassification.from_pretrained('yiyanghkust/finbert-tone',num_labels=3)\n",
    "tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-tone')\n",
    "nlp = pipeline(\"sentiment-analysis\", model=finbert, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f0b9c074",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'Negative', 'score': 0.9966173768043518},\n",
       " {'label': 'Positive', 'score': 1.0},\n",
       " {'label': 'Negative', 'score': 0.9999710321426392},\n",
       " {'label': 'Neutral', 'score': 0.9889442920684814},\n",
       " {'label': 'Neutral', 'score': 0.8356247544288635},\n",
       " {'label': 'Neutral', 'score': 0.9892505407333374}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [\"there is a shortage of capital, and we need extra financing\",  \n",
    "             \"growth is strong and we have plenty of liquidity\", \n",
    "             \"there are doubts about our finances\", \n",
    "             \"profits are flat\",\n",
    "             \"all of our technology systems are vulnerable to disability or failures due to hacking, viruses\",\n",
    "            \"the company cannot be certain that all of its systems are entirely free from vulnerability to attack or other technological difficulties or failures.\"]\n",
    "results = nlp(sentences)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d87847",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "50d539a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83bcbd2051444eb390ba4ab2d5b58bff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/836k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51adcb23183f4ef5a1a3d181ebce1e94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/482k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3310501ce9d448dd9b8623ff31be0266",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/727 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fe880552e6443afa17b20ffb02cdb3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/334M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at SynamicTechnologies/CYBERT were not used when initializing RobertaModel: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at SynamicTechnologies/CYBERT and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained(\"SynamicTechnologies/CYBERT\")\n",
    "model = RobertaModel.from_pretrained(\"SynamicTechnologies/CYBERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54819d9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "454cb049",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "617ee56b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "#tokenizer = RobertaTokenizer.from_pretrained(\"ehsanaghaei/SecureBERT\")\n",
    "#model = RobertaModel.from_pretrained(\"ehsanaghaei/SecureBERT\")\n",
    "\n",
    "#tokenizer = BertTokenizer.from_pretrained('yiyanghkust/finbert-pretrain')\n",
    "#model = BertModel.from_pretrained('yiyanghkust/finbert-pretrain')\n",
    "\n",
    "pipe = pipeline(\"feature-extraction\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "161571aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence0 = 'The commercial production of fully electric vehicles that meets consumers’ range and performance expectations requires substantial design, engineering, and integration work on almost every system of our vehicles.'\n",
    "sentence1 = 'Since the first quarter of 2020, there has been a worldwide impact from the COVID-19 pandemic. Government regulations and shifting social behaviors have, at times, limited or closed non-essential transportation, government functions, business activities and person-to-person interactions. Global trade conditions and consumer trends that originated during the pandemic continue to persist and may also have long-lasting adverse impact on us and our industries independently of the progress of the pandemic.'\n",
    "sentence2 = 'Our products contain thousands of parts purchased globally from hundreds of suppliers, including single-source direct suppliers, which exposes us to multiple potential sources of component shortages.'\n",
    "sentence3 = 'Cost of automotive leasing revenue increased $531 million, or 54%, in the year ended December 31, 2022 as compared to the year ended December 31, 2021, primarily due to an increase in cumulative vehicles under our direct operating lease program and an increase in direct sales-type leasing cost of revenues from more activities in the current year.'\n",
    "\n",
    "sentence4 = 'major breach of our network security and systems could have negative consequences for our business and future prospects including possible fines penalties and damages reduced customer demand for our vehicles and harm to our reputation and brand'\n",
    "sentence5 = 'There may be losses or unauthorized access to or releases of confidential information, including personally identifiable information, that could subject the Company to significant reputational, financial, legal and operational consequences.'\n",
    "\n",
    "sentence6 = 'An adversary may abuse configurations where an application has the setuid or setgid bits set in order to get code running in a different (and possibly more privileged) user’s context. On Linux or macOS, when the setuid or setgid bits are set for an application binary, the application will run with the privileges of the owning user or group respectively. Normally an application is run in the current user’s context, regardless of which user or group owns the application. However, there are instances where programs need to be executed in an elevated context to function properly, but the user running them may not have the specific required privileges.'\n",
    "sentence7 = 'An adversary may compress and/or encrypt data that is collected prior to exfiltration. Compressing the data can help to obfuscate the collected data and minimize the amount of data sent over the network. Encryption can be used to hide information that is being exfiltrated from detection or make exfiltration less conspicuous upon inspection by a defender.'\n",
    "\n",
    "vec = []\n",
    "vec.append(np.mean(pipe(sentence0)[0], axis = 0))\n",
    "vec.append(np.mean(pipe(sentence1)[0], axis = 0))\n",
    "vec.append(np.mean(pipe(sentence2)[0], axis = 0))\n",
    "vec.append(np.mean(pipe(sentence3)[0], axis = 0))\n",
    "vec.append(np.mean(pipe(sentence4)[0], axis = 0))\n",
    "vec.append(np.mean(pipe(sentence5)[0], axis = 0))\n",
    "vec.append(np.mean(pipe(sentence6)[0], axis = 0))\n",
    "vec.append(np.mean(pipe(sentence7)[0], axis = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "e85119fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Not cyber 1</th>\n",
       "      <th>Not cyber 2</th>\n",
       "      <th>Not cyber 3</th>\n",
       "      <th>Not cyber 4</th>\n",
       "      <th>Cyber 1</th>\n",
       "      <th>Cyber 2</th>\n",
       "      <th>Cyber reference 1</th>\n",
       "      <th>Cyber reference 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Not cyber 1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Not cyber 2</th>\n",
       "      <td>0.724423</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Not cyber 3</th>\n",
       "      <td>0.799820</td>\n",
       "      <td>0.754405</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Not cyber 4</th>\n",
       "      <td>0.695063</td>\n",
       "      <td>0.774797</td>\n",
       "      <td>0.671789</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cyber 1</th>\n",
       "      <td>0.740045</td>\n",
       "      <td>0.750979</td>\n",
       "      <td>0.708810</td>\n",
       "      <td>0.676663</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cyber 2</th>\n",
       "      <td>0.646181</td>\n",
       "      <td>0.719096</td>\n",
       "      <td>0.678037</td>\n",
       "      <td>0.657993</td>\n",
       "      <td>0.808613</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cyber reference 1</th>\n",
       "      <td>0.656976</td>\n",
       "      <td>0.718740</td>\n",
       "      <td>0.636582</td>\n",
       "      <td>0.673644</td>\n",
       "      <td>0.641110</td>\n",
       "      <td>0.644208</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cyber reference 2</th>\n",
       "      <td>0.654926</td>\n",
       "      <td>0.742188</td>\n",
       "      <td>0.673326</td>\n",
       "      <td>0.667351</td>\n",
       "      <td>0.689039</td>\n",
       "      <td>0.742805</td>\n",
       "      <td>0.842941</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Not cyber 1  Not cyber 2   Not cyber 3   Not cyber 4  \\\n",
       "Not cyber 1                NaN          NaN           NaN           NaN   \n",
       "Not cyber 2           0.724423          NaN           NaN           NaN   \n",
       " Not cyber 3          0.799820     0.754405           NaN           NaN   \n",
       " Not cyber 4          0.695063     0.774797      0.671789           NaN   \n",
       "Cyber 1               0.740045     0.750979      0.708810      0.676663   \n",
       "Cyber 2               0.646181     0.719096      0.678037      0.657993   \n",
       "Cyber reference 1     0.656976     0.718740      0.636582      0.673644   \n",
       "Cyber reference 2     0.654926     0.742188      0.673326      0.667351   \n",
       "\n",
       "                    Cyber 1   Cyber 2  Cyber reference 1  Cyber reference 2  \n",
       "Not cyber 1             NaN       NaN                NaN                NaN  \n",
       "Not cyber 2             NaN       NaN                NaN                NaN  \n",
       " Not cyber 3            NaN       NaN                NaN                NaN  \n",
       " Not cyber 4            NaN       NaN                NaN                NaN  \n",
       "Cyber 1                 NaN       NaN                NaN                NaN  \n",
       "Cyber 2            0.808613       NaN                NaN                NaN  \n",
       "Cyber reference 1  0.641110  0.644208                NaN                NaN  \n",
       "Cyber reference 2  0.689039  0.742805           0.842941                NaN  "
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy import spatial\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "sim = []\n",
    "for i in range(len(vec)):\n",
    "    for j in range(len(vec)):\n",
    "        if i <= j:\n",
    "            sim.append(np.nan)\n",
    "            continue\n",
    "        cos_distance = spatial.distance.cosine(vec[i], vec[j])\n",
    "        similarity = 1-abs(cos_distance)\n",
    "        sim.append(similarity)\n",
    "\n",
    "idx = ['Not cyber 1', 'Not cyber 2', ' Not cyber 3',' Not cyber 4',\n",
    "       'Cyber 1', 'Cyber 2',\n",
    "       'Cyber reference 1', 'Cyber reference 2']\n",
    "pd.DataFrame(np.array(sim).reshape(len(vec),-1), columns = idx, index = idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc4a2bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
